{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NLP Assignment 1 â€” Enhancing Figurative Language Recognition Using POS Tagging\n",
        "\n",
        "Author: Tohuto Sema (Roll: 21CS10072)\n",
        "\n",
        "This notebook contains step-by-step code for:\n",
        "- Task 1: Implementing an HMM POS tagger (Viterbi) from scratch using NLTK Treebank.\n",
        "- Task 2: Baseline figurative-language classifier (text-only features).\n",
        "- Task 3: Improved classifier that augments text features with POS tag sequences from Task 1.\n",
        "- Task 4: Reporting and an interactive prediction utility.\n",
        "\n",
        "Run cells sequentially. Cells that install packages can be skipped if you already installed dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Install dependencies\n",
        "\n",
        "Run this cell once in the notebook environment. If you are running in a conda/venv, you can skip and install from terminal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (run once)\n",
        "!pip install --upgrade pip\n",
        "!pip install --quiet nltk scikit-learn datasets huggingface_hub\n",
        "# Note: 'datasets' will be used to download ColumbiaNLP/V-FLUTE if you have access.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Imports and NLTK data download\n",
        "Download the Treebank corpus and universal tagset (this is required for Task 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cwd = c:\\Users\\Benjamin Doley\\Documents\\NLP_Assignment_1_21CS10072\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to C:\\Users\\Benjamin\n",
            "[nltk_data]     Doley\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to C:\\Users\\Benjamin\n",
            "[nltk_data]     Doley\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Treebank sentences: 3914\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print('cwd =', os.getcwd())\n",
        "from collections import Counter\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict, Iterable, Optional\n",
        "\n",
        "import nltk\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "from nltk.corpus import treebank\n",
        "print('Treebank sentences:', len(treebank.tagged_sents(tagset='universal')))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) HMM + Viterbi implementation (Task 1)\n",
        "This cell defines the HMM dataclass, training and Viterbi decoding functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "START = \"<s>\"\n",
        "STOP = \"</s>\"\n",
        "UNK = \"<unk>\"\n",
        "@dataclass\n",
        "class HMMParams:\n",
        "    tag_to_id: Dict[str, int]\n",
        "    id_to_tag: List[str]\n",
        "    word_to_id: Dict[str, int]\n",
        "    id_to_word: List[str]\n",
        "    log_trans: List[List[float]]\n",
        "    log_emit: List[Dict[int, float]]\n",
        "    log_start: List[float]\n",
        "    log_stop: List[float]\n",
        "\n",
        "def add1_log_prob(count: int, total: int, vocab: int) -> float:\n",
        "    return math.log(count + 1) - math.log(total + vocab)\n",
        "\n",
        "def build_vocab(tagged_sents: List[List[Tuple[str,str]]], min_freq:int=1):\n",
        "    wc = Counter(w.lower() for sent in tagged_sents for (w,_) in sent)\n",
        "    words = [w for w,c in wc.items() if c >= min_freq]\n",
        "    word_to_id = {UNK:0, **{w:i+1 for i,w in enumerate(sorted(words))}}\n",
        "    id_to_word = [None]*len(word_to_id)\n",
        "    for w,i in word_to_id.items():\n",
        "        id_to_word[i] = w\n",
        "    tags = sorted({t for sent in tagged_sents for (_,t) in sent})\n",
        "    tag_to_id = {t:i for i,t in enumerate(tags)}\n",
        "    id_to_tag = tags[:]\n",
        "    return word_to_id, id_to_word, tag_to_id, id_to_tag\n",
        "\n",
        "def train_hmm(tagged_train: List[List[Tuple[str,str]]], tagged_dev: Optional[List[List[Tuple[str,str]]]]=None) -> HMMParams:\n",
        "    word_to_id, id_to_word, tag_to_id, id_to_tag = build_vocab(tagged_train)\n",
        "    T = len(tag_to_id)\n",
        "    trans = [[0 for _ in range(T)] for _ in range(T)]\n",
        "    emit: List[Counter] = [Counter() for _ in range(T)]\n",
        "    start = [0 for _ in range(T)]\n",
        "    stop = [0 for _ in range(T)]\n",
        "    for sent in tagged_train:\n",
        "        if not sent: continue\n",
        "        t0 = tag_to_id[sent[0][1]]\n",
        "        start[t0] += 1\n",
        "        for i, (w,t) in enumerate(sent):\n",
        "            tid = tag_to_id[t]\n",
        "            wid = word_to_id.get(w.lower(), word_to_id[UNK])\n",
        "            emit[tid][wid] += 1\n",
        "            if i+1 < len(sent):\n",
        "                tid_next = tag_to_id[sent[i+1][1]]\n",
        "                trans[tid][tid_next] += 1\n",
        "            else:\n",
        "                stop[tid] += 1\n",
        "    total_start = sum(start)\n",
        "    log_start = [add1_log_prob(start[i], total_start, T) for i in range(T)]\n",
        "    total_stop = sum(stop)\n",
        "    log_stop = [add1_log_prob(stop[i], total_stop, T) for i in range(T)]\n",
        "    log_trans = []\n",
        "    for i in range(T):\n",
        "        row_total = sum(trans[i])\n",
        "        log_row = [add1_log_prob(trans[i][j], row_total, T) for j in range(T)]\n",
        "        log_trans.append(log_row)\n",
        "    log_emit: List[Dict[int,float]] = []\n",
        "    for tid in range(T):\n",
        "        total = sum(emit[tid].values())\n",
        "        V = len(word_to_id)\n",
        "        tag_emit = {}\n",
        "        for wid, c in emit[tid].items():\n",
        "            tag_emit[wid] = add1_log_prob(c, total, V)\n",
        "        tag_emit[word_to_id[UNK]] = add1_log_prob(0, total, V)\n",
        "        log_emit.append(tag_emit)\n",
        "    return HMMParams(tag_to_id, id_to_tag, word_to_id, id_to_word, log_trans, log_emit, log_start, log_stop)\n",
        "\n",
        "def viterbi_decode(words: List[str], params: HMMParams) -> List[str]:\n",
        "    T = len(params.id_to_tag)\n",
        "    n = len(words)\n",
        "    dp = [[-1e18]*T for _ in range(n)]\n",
        "    bp = [[-1]*T for _ in range(n)]\n",
        "    def emit_log(tid:int, wid:int) -> float:\n",
        "        d = params.log_emit[tid]\n",
        "        return d.get(wid, d[params.word_to_id[UNK]])\n",
        "    wid0 = params.word_to_id.get(words[0].lower(), params.word_to_id[UNK])\n",
        "    for t in range(T):\n",
        "        dp[0][t] = params.log_start[t] + emit_log(t, wid0)\n",
        "        bp[0][t] = -1\n",
        "    for i in range(1, n):\n",
        "        wid = params.word_to_id.get(words[i].lower(), params.word_to_id[UNK])\n",
        "        for t in range(T):\n",
        "            best = -1e18\n",
        "            arg = -1\n",
        "            e = emit_log(t, wid)\n",
        "            for tprev in range(T):\n",
        "                score = dp[i-1][tprev] + params.log_trans[tprev][t] + e\n",
        "                if score > best:\n",
        "                    best, arg = score, tprev\n",
        "            dp[i][t] = best\n",
        "            bp[i][t] = arg\n",
        "    best_final = -1e18\n",
        "    last_tag = -1\n",
        "    for t in range(T):\n",
        "        score = dp[n-1][t] + params.log_stop[t]\n",
        "        if score > best_final:\n",
        "            best_final, last_tag = score, t\n",
        "    tags_idx = [0]*n\n",
        "    tags_idx[-1] = last_tag\n",
        "    for i in range(n-2, -1, -1):\n",
        "        tags_idx[i] = bp[i+1][tags_idx[i+1]]\n",
        "    return [params.id_to_tag[t] for t in tags_idx]\n",
        "\n",
        "def tag_sentence(sentence: List[str], params: HMMParams) -> List[Tuple[str,str]]:\n",
        "    tags = viterbi_decode(sentence, params)\n",
        "    return list(zip(sentence, tags))\n",
        "\n",
        "def accuracy(tagged_gold: List[List[Tuple[str,str]]], params: HMMParams) -> float:\n",
        "    correct = total = 0\n",
        "    for sent in tagged_gold:\n",
        "        words = [w for w,_ in sent]\n",
        "        gold = [t for _,t in sent]\n",
        "        pred = viterbi_decode(words, params)\n",
        "        for g,p in zip(gold, pred):\n",
        "            total += 1\n",
        "            if g == p: correct += 1\n",
        "    return correct / max(1,total)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Train the HMM on NLTK Treebank and evaluate\n",
        "This cell loads Treebank, splits train/dev/test, trains HMM and prints accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train sents: 3131 Dev sents: 391 Test sents: 392\n",
            "Training HMM... (this may take a few seconds)\n",
            "Evaluating...\n",
            "Dev Accuracy: 0.8912\n",
            "Test Accuracy: 0.8834\n"
          ]
        }
      ],
      "source": [
        "# Load Treebank tagged sentences (universal tagset)\n",
        "tagged_sents = treebank.tagged_sents(tagset='universal')\n",
        "n = len(tagged_sents)\n",
        "train_sents = tagged_sents[:int(0.8*n)]\n",
        "dev_sents = tagged_sents[int(0.8*n):int(0.9*n)]\n",
        "test_sents = tagged_sents[int(0.9*n):]\n",
        "print('Train sents:', len(train_sents), 'Dev sents:', len(dev_sents), 'Test sents:', len(test_sents))\n",
        "print('Training HMM... (this may take a few seconds)')\n",
        "hmm = train_hmm(train_sents)\n",
        "print('Evaluating...')\n",
        "dev_acc = accuracy(dev_sents, hmm)\n",
        "test_acc = accuracy(test_sents, hmm)\n",
        "print(f'Dev Accuracy: {dev_acc:.4f}')\n",
        "print(f'Test Accuracy: {test_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Baseline classifier (Task 2) and helper functions\n",
        "Defines dataset loader (HF V-FLUTE) and classifier training function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Benjamin Doley\\Documents\\NLP_Assignment_1_21CS10072\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "except Exception:\n",
        "    load_dataset = None\n",
        "\n",
        "def load_flute(split: str = 'train', use_hf: bool = True, local_csv: Optional[str] = None):\n",
        "    if use_hf and load_dataset is not None:\n",
        "        ds = load_dataset('ColumbiaNLP/V-FLUTE', split=split)\n",
        "        texts = [(rec['claim'] or '') + ' ' + (rec['explanation'] or '') for rec in ds]\n",
        "        labels = [rec['phenomenon'] for rec in ds]\n",
        "        label_names = sorted(set(labels))\n",
        "        name_to_id = {n:i for i,n in enumerate(label_names)}\n",
        "        y = [name_to_id[l] for l in labels]\n",
        "        return texts, y, label_names\n",
        "    else:\n",
        "        if local_csv is None:\n",
        "            raise RuntimeError('Provide local_csv path if not using HF datasets.')\n",
        "        import csv\n",
        "        texts, labels = [], []\n",
        "        with open(local_csv, newline='', encoding='utf-8') as f:\n",
        "            reader = csv.DictReader(f)\n",
        "            for r in reader:\n",
        "                texts.append((r.get('claim','') + ' ' + r.get('explanation','')).strip())\n",
        "                labels.append(r['phenomenon'])\n",
        "        label_names = sorted(set(labels))\n",
        "        name_to_id = {n:i for i,n in enumerate(label_names)}\n",
        "        y = [name_to_id[l] for l in labels]\n",
        "        return texts, y, label_names\n",
        "\n",
        "def train_baseline_classifier(texts: List[str], y: List[int], cls: str = 'svm'):\n",
        "    if cls == 'svm':\n",
        "        model = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_features=50000, ngram_range=(1,2))),\n",
        "            ('clf', LinearSVC())\n",
        "        ])\n",
        "    else:\n",
        "        model = Pipeline([\n",
        "            ('tfidf', TfidfVectorizer(max_features=50000, ngram_range=(1,2))),\n",
        "            ('clf', MultinomialNB())\n",
        "        ])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(texts, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return model, (y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Improved classifier (Task 3) that uses POS sequences\n",
        "This defines helper to produce POS sequences and trains the combined pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "def gen_pos_sequence(text: str, hmm: HMMParams) -> str:\n",
        "    words = [w for w in text.split() if w.strip()]\n",
        "    if not words:\n",
        "        return ''\n",
        "    tags = viterbi_decode(words, hmm)\n",
        "    return ' '.join(tags)\n",
        "\n",
        "def train_improved_classifier(texts: List[str], y: List[int], hmm: HMMParams, cls: str = 'svm'):\n",
        "    def pos_seq_transform(batch: Iterable[str]):\n",
        "        return [gen_pos_sequence(t, hmm) for t in batch]\n",
        "\n",
        "    text_ft = ('text_ft', Pipeline([\n",
        "        ('id', FunctionTransformer(lambda x: x, validate=False)),\n",
        "        ('tfidf', TfidfVectorizer(max_features=50000, ngram_range=(1,2)))\n",
        "    ]))\n",
        "    pos_ft = ('pos_ft', Pipeline([\n",
        "        ('pos', FunctionTransformer(pos_seq_transform, validate=False)),\n",
        "        ('tfidf', TfidfVectorizer(ngram_range=(1,3)))\n",
        "    ]))\n",
        "    features = FeatureUnion([text_ft, pos_ft])\n",
        "    clf = LinearSVC() if cls == 'svm' else MultinomialNB()\n",
        "    model = Pipeline([\n",
        "        ('feats', features),\n",
        "        ('clf', clf)\n",
        "    ])\n",
        "    X_train, X_test, y_train, y_test = train_test_split(texts, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return model, (y_test, y_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Train classifiers (if FLUTE available)\n",
        "This cell attempts to load FLUTE from Hugging Face. If the dataset is gated, you'll need to accept the dataset page on HF and login (see notes below).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded FLUTE examples: 4578 labels: 5\n",
            "\n",
            "=== Baseline (No POS) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       humor       0.99      1.00      1.00       404\n",
            "       idiom       1.00      1.00      1.00        34\n",
            "    metaphor       1.00      0.98      0.99       229\n",
            "     sarcasm       0.99      0.99      0.99       166\n",
            "      simile       1.00      1.00      1.00        83\n",
            "\n",
            "    accuracy                           0.99       916\n",
            "   macro avg       1.00      1.00      1.00       916\n",
            "weighted avg       0.99      0.99      0.99       916\n",
            "\n",
            "Confusion Matrix:\n",
            " [[404   0   0   0   0]\n",
            " [  0  34   0   0   0]\n",
            " [  3   0 225   1   0]\n",
            " [  1   0   0 165   0]\n",
            " [  0   0   0   0  83]]\n",
            "\n",
            "=== Improved (With POS Features) ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       humor       0.99      1.00      0.99       404\n",
            "       idiom       1.00      1.00      1.00        34\n",
            "    metaphor       1.00      0.98      0.99       229\n",
            "     sarcasm       0.99      0.99      0.99       166\n",
            "      simile       1.00      1.00      1.00        83\n",
            "\n",
            "    accuracy                           0.99       916\n",
            "   macro avg       1.00      0.99      0.99       916\n",
            "weighted avg       0.99      0.99      0.99       916\n",
            "\n",
            "Confusion Matrix:\n",
            " [[404   0   0   0   0]\n",
            " [  0  34   0   0   0]\n",
            " [  4   0 224   1   0]\n",
            " [  2   0   0 164   0]\n",
            " [  0   0   0   0  83]]\n"
          ]
        }
      ],
      "source": [
        "texts = y = label_names = None\n",
        "try:\n",
        "    texts, y, label_names = load_flute(split='train', use_hf=True)\n",
        "    print('Loaded FLUTE examples:', len(texts), 'labels:', len(label_names))\n",
        "except Exception as e:\n",
        "    print('[WARN] Could not load FLUTE from HF:', e)\n",
        "    print('If you have the dataset CSVs locally, call load_flute(use_hf=False, local_csv=\"path.csv\")')\n",
        "\n",
        "if texts:\n",
        "    base_model, (y_true_base, y_pred_base) = train_baseline_classifier(texts, y, cls='svm')\n",
        "    print('\\n=== Baseline (No POS) ===')\n",
        "    print(classification_report(y_true_base, y_pred_base, target_names=label_names))\n",
        "    print('Confusion Matrix:\\n', confusion_matrix(y_true_base, y_pred_base))\n",
        "    if hmm is not None:\n",
        "        pos_model, (y_true_pos, y_pred_pos) = train_improved_classifier(texts, y, hmm, cls='svm')\n",
        "        print('\\n=== Improved (With POS Features) ===')\n",
        "        print(classification_report(y_true_pos, y_pred_pos, target_names=label_names))\n",
        "        print('Confusion Matrix:\\n', confusion_matrix(y_true_pos, y_pred_pos))\n",
        "else:\n",
        "    print('[INFO] FLUTE dataset not available. Skipping classifiers.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Interactive prediction helper\n",
        "Call `predict_sentence(sentence, use_pos=False)` to get a prediction. This cell defines the helper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_sentence(sentence: str, use_pos: bool = False):\n",
        "    if texts is None:\n",
        "        print('[WARN] No FLUTE dataset/model available. Train or load models first.')\n",
        "        return None\n",
        "    model = pos_model if use_pos and 'pos_model' in globals() else base_model\n",
        "    pred_id = model.predict([sentence])[0]\n",
        "    pred_label = label_names[pred_id]\n",
        "    print('Predicted class:', pred_label)\n",
        "    return pred_label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes & Hugging Face access\n",
        "- The ColumbiaNLP/V-FLUTE dataset on Hugging Face may be gated. Visit the dataset page and click **Accept** to gain access.\n",
        "- To authenticate in a terminal, run `huggingface-cli login` and provide your token, or configure `HF_TOKEN` as shown on the Hugging Face docs.\n",
        "- If you cannot access HF, export the dataset to CSVs (train/validation/test) and provide the `local_csv` path to `load_flute(..., use_hf=False, local_csv=...)`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How to use this notebook\n",
        "1. Run the cells in order.\n",
        "2. If FLUTE loading fails, provide a local CSV and call `load_flute(use_hf=False, local_csv='data/flute_train.csv')`.\n",
        "3. Use `predict_sentence('your text', use_pos=True)` to test the POS-improved model (if trained).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
